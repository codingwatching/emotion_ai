#!/usr/bin/env python3
"""
Validation Script for Gemini 2.5 Tool Calling Fixes
==================================================

This script validates the comprehensive fixes implemented to address:
1. Random Gemini 2.5 tool call failures and response cutoffs
2. Chat history persistence issues
3. Concurrent tool calling problems
4. Session recovery mechanisms

Usage:
    python validate_gemini_fixes.py
"""

import asyncio
import os
import sys
import logging
import traceback
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

# Add the backend directory to Python path
backend_dir = Path(__file__).parent
sys.path.insert(0, str(backend_dir))

# Configure logging for validation
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('gemini_fixes_validator')

class GeminiFixes Validator:
    """Comprehensive validator for Gemini 2.5 tool calling fixes"""

    def __init__(self):
        self.test_results = {}
        self.validation_errors = []

    async def run_all_validations(self) -> Dict[str, Any]:
        """Run comprehensive validation of all implemented fixes"""
        logger.info("🧪 Starting comprehensive validation of Gemini 2.5 fixes...")

        validations = [
            ("Configuration Validation", self.validate_configuration),
            ("MCP Bridge Enhancements", self.validate_mcp_bridge_enhancements),
            ("Conversation Processing Fixes", self.validate_conversation_processing),
            ("Session Management", self.validate_session_management),
            ("Error Handling", self.validate_error_handling),
            ("Integration Test", self.validate_integration),
        ]

        for test_name, test_func in validations:
            try:
                logger.info(f"🔍 Running: {test_name}")
                result = await test_func()
                self.test_results[test_name] = result
                status = "✅ PASS" if result['passed'] else "❌ FAIL"
                logger.info(f"{status} {test_name}: {result['message']}")

                if not result['passed']:
                    self.validation_errors.append(f"{test_name}: {result['message']}")

            except Exception as e:
                error_msg = f"Exception in {test_name}: {str(e)}"
                logger.error(f"💥 {error_msg}")
                logger.error(traceback.format_exc())
                self.test_results[test_name] = {
                    'passed': False,
                    'message': error_msg,
                    'exception': str(e)
                }
                self.validation_errors.append(error_msg)

        return self._generate_final_report()

    async def validate_configuration(self) -> Dict[str, Any]:
        """Validate that configuration parameters are properly loaded"""
        try:
            # Load environment variables
            from dotenv import load_dotenv
            load_dotenv()

            # Check required configuration parameters
            config_checks = {
                'TOOL_CALL_MAX_RETRIES': os.getenv('TOOL_CALL_MAX_RETRIES', '3'),
                'TOOL_CALL_RETRY_DELAY': os.getenv('TOOL_CALL_RETRY_DELAY', '1.0'),
                'TOOL_CALL_EXPONENTIAL_BACKOFF': os.getenv('TOOL_CALL_EXPONENTIAL_BACKOFF', 'true'),
                'TOOL_CALL_SEQUENTIAL_MODE': os.getenv('TOOL_CALL_SEQUENTIAL_MODE', 'false'),
                'TOOL_CALL_TIMEOUT': os.getenv('TOOL_CALL_TIMEOUT', '30.0'),
                'CONVERSATION_PERSISTENCE_RETRIES': os.getenv('CONVERSATION_PERSISTENCE_RETRIES', '2'),
                'SESSION_RECOVERY_ENABLED': os.getenv('SESSION_RECOVERY_ENABLED', 'true')
            }

            # Validate configuration values
            validation_issues = []

            # Validate numeric parameters
            try:
                max_retries = int(config_checks['TOOL_CALL_MAX_RETRIES'])
                if max_retries < 1 or max_retries > 10:
                    validation_issues.append(f"TOOL_CALL_MAX_RETRIES should be 1-10, got {max_retries}")
            except ValueError:
                validation_issues.append(f"TOOL_CALL_MAX_RETRIES must be integer, got {config_checks['TOOL_CALL_MAX_RETRIES']}")

            try:
                retry_delay = float(config_checks['TOOL_CALL_RETRY_DELAY'])
                if retry_delay < 0.1 or retry_delay > 10.0:
                    validation_issues.append(f"TOOL_CALL_RETRY_DELAY should be 0.1-10.0, got {retry_delay}")
            except ValueError:
                validation_issues.append(f"TOOL_CALL_RETRY_DELAY must be float, got {config_checks['TOOL_CALL_RETRY_DELAY']}")

            try:
                timeout = float(config_checks['TOOL_CALL_TIMEOUT'])
                if timeout < 5.0 or timeout > 120.0:
                    validation_issues.append(f"TOOL_CALL_TIMEOUT should be 5.0-120.0, got {timeout}")
            except ValueError:
                validation_issues.append(f"TOOL_CALL_TIMEOUT must be float, got {config_checks['TOOL_CALL_TIMEOUT']}")

            # Validate boolean parameters
            boolean_params = ['TOOL_CALL_EXPONENTIAL_BACKOFF', 'TOOL_CALL_SEQUENTIAL_MODE', 'SESSION_RECOVERY_ENABLED']
            for param in boolean_params:
                value = config_checks[param].lower()
                if value not in ['true', 'false']:
                    validation_issues.append(f"{param} must be 'true' or 'false', got {config_checks[param]}")

            if validation_issues:
                return {
                    'passed': False,
                    'message': f"Configuration validation failed: {'; '.join(validation_issues)}",
                    'config_values': config_checks,
                    'issues': validation_issues
                }

            return {
                'passed': True,
                'message': f"Configuration validation passed - all {len(config_checks)} parameters valid",
                'config_values': config_checks
            }

        except Exception as e:
            return {
                'passed': False,
                'message': f"Configuration validation error: {str(e)}",
                'exception': str(e)
            }

    async def validate_mcp_bridge_enhancements(self) -> Dict[str, Any]:
        """Validate MCP bridge enhancements for tool calling reliability"""
        try:
            # Import and inspect the enhanced MCP bridge
            from mcp_to_gemini_bridge import MCPGeminiBridge, ToolExecutionResult
            import inspect

            # Check that enhanced methods exist
            required_methods = [
                '_execute_function_call_with_retry',
                '_execute_single_function_call'
            ]

            missing_methods = []
            for method_name in required_methods:
                if not hasattr(MCPGeminiBridge, method_name):
                    missing_methods.append(method_name)

            if missing_methods:
                return {
                    'passed': False,
                    'message': f"Missing enhanced methods: {missing_methods}",
                    'missing_methods': missing_methods
                }

            # Check method signatures
            method_checks = {}

            # Check retry method signature
            retry_method = getattr(MCPGeminiBridge, '_execute_function_call_with_retry')
            retry_sig = inspect.signature(retry_method)
            method_checks['retry_method'] = {
                'params': list(retry_sig.parameters.keys()),
                'has_async': inspect.iscoroutinefunction(retry_method)
            }

            # Check single execution method signature
            single_method = getattr(MCPGeminiBridge, '_execute_single_function_call')
            single_sig = inspect.signature(single_method)
            method_checks['single_method'] = {
                'params': list(single_sig.parameters.keys()),
                'has_async': inspect.iscoroutinefunction(single_method)
            }

            # Validate expected parameters exist
            expected_retry_params = ['self', 'function_call', 'user_id']
            expected_single_params = ['self', 'function_call', 'user_id']

            retry_param_issues = []
            for param in expected_retry_params:
                if param not in method_checks['retry_method']['params']:
                    retry_param_issues.append(f"Missing parameter: {param}")

            single_param_issues = []
            for param in expected_single_params:
                if param not in method_checks['single_method']['params']:
                    single_param_issues.append(f"Missing parameter: {param}")

            # Check ToolExecutionResult structure
            tool_result_fields = ['tool_name', 'success', 'result', 'error', 'execution_time']
            result_instance = ToolExecutionResult("test", True, None)
            missing_fields = []
            for field in tool_result_fields:
                if not hasattr(result_instance, field):
                    missing_fields.append(field)

            issues = []
            if retry_param_issues:
                issues.extend([f"Retry method: {issue}" for issue in retry_param_issues])
            if single_param_issues:
                issues.extend([f"Single method: {issue}" for issue in single_param_issues])
            if missing_fields:
                issues.extend([f"ToolExecutionResult missing: {field}" for field in missing_fields])

            if issues:
                return {
                    'passed': False,
                    'message': f"MCP bridge validation failed: {'; '.join(issues)}",
                    'method_details': method_checks,
                    'issues': issues
                }

            return {
                'passed': True,
                'message': f"MCP bridge enhancements validated - {len(required_methods)} methods enhanced",
                'method_details': method_checks
            }

        except Exception as e:
            return {
                'passed': False,
                'message': f"MCP bridge validation error: {str(e)}",
                'exception': str(e)
            }

    async def validate_conversation_processing(self) -> Dict[str, Any]:
        """Validate enhanced conversation processing functions"""
        try:
            # Import main module and check for enhanced functions
            sys.path.append(str(backend_dir))

            # Check if the enhanced functions exist (they should be in main.py)
            # We'll check by examining the source code since these are internal functions
            main_py_path = backend_dir / 'main.py'

            if not main_py_path.exists():
                return {
                    'passed': False,
                    'message': "main.py not found",
                    'path_checked': str(main_py_path)
                }

            # Read main.py content and check for enhanced functions
            with open(main_py_path, 'r') as f:
                main_content = f.read()

            required_functions = [
                '_get_or_create_chat_session',
                '_process_conversation_with_thinking'
            ]

            missing_functions = []
            for func_name in required_functions:
                if f"async def {func_name}" not in main_content:
                    missing_functions.append(func_name)

            # Check for enhanced error handling patterns
            error_handling_patterns = [
                'conversation_persistence_retries',
                'session_recovery_enabled',
                'max_conversation_retries',
                'recoverable_errors',
                'fallback_response'
            ]

            missing_patterns = []
            for pattern in error_handling_patterns:
                if pattern not in main_content:
                    missing_patterns.append(pattern)

            # Check for enhanced persistence logic
            persistence_patterns = [
                'persist_with_enhanced_logging',
                'attempt in range(conversation_persistence_retries',
                'Enhanced persistence wrapper'
            ]

            missing_persistence = []
            for pattern in persistence_patterns:
                if pattern not in main_content:
                    missing_persistence.append(pattern)

            issues = []
            if missing_functions:
                issues.extend([f"Missing function: {func}" for func in missing_functions])
            if missing_patterns:
                issues.extend([f"Missing error handling: {pattern}" for pattern in missing_patterns])
            if missing_persistence:
                issues.extend([f"Missing persistence logic: {pattern}" for pattern in missing_persistence])

            if issues:
                return {
                    'passed': False,
                    'message': f"Conversation processing validation failed: {'; '.join(issues[:3])}{'...' if len(issues) > 3 else ''}",
                    'issues': issues,
                    'functions_checked': required_functions
                }

            return {
                'passed': True,
                'message': f"Conversation processing enhancements validated - {len(required_functions)} functions enhanced",
                'functions_found': required_functions,
                'patterns_found': len(error_handling_patterns) + len(persistence_patterns)
            }

        except Exception as e:
            return {
                'passed': False,
                'message': f"Conversation processing validation error: {str(e)}",
                'exception': str(e)
            }

    async def validate_session_management(self) -> Dict[str, Any]:
        """Validate enhanced session management capabilities"""
        try:
            # Check main.py for session management enhancements
            main_py_path = backend_dir / 'main.py'

            with open(main_py_path, 'r') as f:
                main_content = f.read()

            # Check for session management patterns
            session_patterns = [
                'active_chat_sessions',
                'session_tool_versions',
                'global_tool_version',
                'needs_new_session',
                'session_recovery_enabled'
            ]

            missing_session_patterns = []
            for pattern in session_patterns:
                if pattern not in main_content:
                    missing_session_patterns.append(pattern)

            # Check for session cleanup logic
            cleanup_patterns = [
                'del active_chat_sessions[session_key]',
                'del session_tool_versions[session_key]',
                'Session cleared for recreation',
                'Failed session cleared'
            ]

            found_cleanup_patterns = []
            for pattern in cleanup_patterns:
                if pattern in main_content:
                    found_cleanup_patterns.append(pattern)

            # Check for version management
            version_patterns = [
                'outdated tools',
                'global_tool_version',
                'session_tool_versions[session_key] = global_tool_version'
            ]

            found_version_patterns = []
            for pattern in version_patterns:
                if pattern in main_content:
                    found_version_patterns.append(pattern)

            issues = []
            if missing_session_patterns:
                issues.extend([f"Missing session pattern: {pattern}" for pattern in missing_session_patterns])

            if len(found_cleanup_patterns) < 2:
                issues.append(f"Insufficient cleanup patterns found: {len(found_cleanup_patterns)}/4")

            if len(found_version_patterns) < 2:
                issues.append(f"Insufficient version management patterns: {len(found_version_patterns)}/3")

            if issues:
                return {
                    'passed': False,
                    'message': f"Session management validation failed: {'; '.join(issues[:2])}{'...' if len(issues) > 2 else ''}",
                    'issues': issues,
                    'cleanup_patterns_found': len(found_cleanup_patterns),
                    'version_patterns_found': len(found_version_patterns)
                }

            return {
                'passed': True,
                'message': f"Session management validated - {len(found_cleanup_patterns)} cleanup + {len(found_version_patterns)} version patterns",
                'cleanup_patterns': len(found_cleanup_patterns),
                'version_patterns': len(found_version_patterns),
                'session_patterns': len(session_patterns) - len(missing_session_patterns)
            }

        except Exception as e:
            return {
                'passed': False,
                'message': f"Session management validation error: {str(e)}",
                'exception': str(e)
            }

    async def validate_error_handling(self) -> Dict[str, Any]:
        """Validate comprehensive error handling improvements"""
        try:
            # Check both main.py and mcp_to_gemini_bridge.py for error handling
            files_to_check = [
                ('main.py', backend_dir / 'main.py'),
                ('mcp_to_gemini_bridge.py', backend_dir / 'mcp_to_gemini_bridge.py')
            ]

            error_handling_results = {}

            for file_name, file_path in files_to_check:
                if not file_path.exists():
                    error_handling_results[file_name] = {
                        'exists': False,
                        'error': f"File not found: {file_path}"
                    }
                    continue

                with open(file_path, 'r') as f:
                    content = f.read()

                # Check for comprehensive error handling patterns
                error_patterns = {
                    'retry_logic': ['for attempt in range', 'retry attempt', 'max.*retries'],
                    'timeout_handling': ['asyncio.wait_for', 'timeout', 'TimeoutError'],
                    'fallback_mechanisms': ['fallback', 'recovery', 'degradation'],
                    'error_classification': ['recoverable', 'is_recoverable', 'error_str'],
                    'logging_enhancements': ['logger.error', 'logger.warning', 'logger.info']
                }

                found_patterns = {}
                for category, patterns in error_patterns.items():
                    found_patterns[category] = []
                    for pattern in patterns:
                        if any(p in content.lower() for p in [pattern.lower()]):
                            found_patterns[category].append(pattern)

                error_handling_results[file_name] = {
                    'exists': True,
                    'patterns_found': found_patterns,
                    'total_patterns': sum(len(patterns) for patterns in found_patterns.values())
                }

            # Analyze results
            total_files_checked = len([r for r in error_handling_results.values() if r['exists']])
            total_patterns_found = sum(r.get('total_patterns', 0) for r in error_handling_results.values() if r['exists'])

            if total_files_checked < 2:
                return {
                    'passed': False,
                    'message': f"Could not check all files - only {total_files_checked}/2 files found",
                    'file_results': error_handling_results
                }

            if total_patterns_found < 8:  # Minimum threshold for comprehensive error handling
                return {
                    'passed': False,
                    'message': f"Insufficient error handling patterns found: {total_patterns_found}/8+ expected",
                    'file_results': error_handling_results,
                    'total_patterns': total_patterns_found
                }

            return {
                'passed': True,
                'message': f"Error handling validated - {total_patterns_found} patterns across {total_files_checked} files",
                'file_results': error_handling_results,
                'total_patterns': total_patterns_found
            }

        except Exception as e:
            return {
                'passed': False,
                'message': f"Error handling validation error: {str(e)}",
                'exception': str(e)
            }

    async def validate_integration(self) -> Dict[str, Any]:
        """Validate overall integration of all fixes"""
        try:
            # This is a comprehensive integration check
            integration_checks = {}

            # 1. Check that imports work correctly
            try:
                from dotenv import load_dotenv
                load_dotenv()
                integration_checks['env_loading'] = True
            except Exception as e:
                integration_checks['env_loading'] = False
                integration_checks['env_error'] = str(e)

            # 2. Check MCP bridge import and configuration constants
            try:
                from mcp_to_gemini_bridge import (
                    TOOL_CALL_MAX_RETRIES,
                    TOOL_CALL_RETRY_DELAY,
                    TOOL_CALL_EXPONENTIAL_BACKOFF
                )
                integration_checks['mcp_bridge_imports'] = True
                integration_checks['config_constants'] = {
                    'max_retries': TOOL_CALL_MAX_RETRIES,
                    'retry_delay': TOOL_CALL_RETRY_DELAY,
                    'exponential_backoff': TOOL_CALL_EXPONENTIAL_BACKOFF
                }
            except Exception as e:
                integration_checks['mcp_bridge_imports'] = False
                integration_checks['mcp_bridge_error'] = str(e)

            # 3. Check that main.py functions reference the correct configuration
            main_py_path = backend_dir / 'main.py'
            if main_py_path.exists():
                with open(main_py_path, 'r') as f:
                    main_content = f.read()

                config_usage = [
                    'CONVERSATION_PERSISTENCE_RETRIES',
                    'SESSION_RECOVERY_ENABLED',
                    'conversation_persistence_retries',
                    'session_recovery_enabled'
                ]

                config_usage_found = []
                for usage in config_usage:
                    if usage in main_content:
                        config_usage_found.append(usage)

                integration_checks['main_config_usage'] = config_usage_found
                integration_checks['main_config_ratio'] = len(config_usage_found) / len(config_usage)

            # 4. Validate the fixes work together cohesively
            cohesion_issues = []

            if not integration_checks.get('env_loading', False):
                cohesion_issues.append("Environment loading failed")

            if not integration_checks.get('mcp_bridge_imports', False):
                cohesion_issues.append("MCP bridge configuration imports failed")

            if integration_checks.get('main_config_ratio', 0) < 0.75:
                cohesion_issues.append("Main module doesn't properly use configuration")

            # 5. Check for circular dependencies or import issues
            try:
                # Try importing the main components to check for issues
                import sys
                temp_modules = []

                # Save current modules
                original_modules = sys.modules.copy()

                # Clear potentially problematic modules
                modules_to_clear = [m for m in sys.modules.keys() if 'mcp_to_gemini_bridge' in m or 'main' in m]
                for module in modules_to_clear:
                    if module in sys.modules:
                        temp_modules.append((module, sys.modules[module]))
                        del sys.modules[module]

                # Try re-importing
                from mcp_to_gemini_bridge import MCPGeminiBridge
                integration_checks['import_test'] = True

                # Restore modules
                for module_name, module_obj in temp_modules:
                    sys.modules[module_name] = module_obj

            except Exception as e:
                integration_checks['import_test'] = False
                integration_checks['import_error'] = str(e)
                cohesion_issues.append(f"Import test failed: {str(e)}")

            if cohesion_issues:
                return {
                    'passed': False,
                    'message': f"Integration validation failed: {'; '.join(cohesion_issues)}",
                    'integration_details': integration_checks,
                    'cohesion_issues': cohesion_issues
                }

            return {
                'passed': True,
                'message': f"Integration validation passed - all components work together cohesively",
                'integration_details': integration_checks,
                'config_usage_ratio': integration_checks.get('main_config_ratio', 0)
            }

        except Exception as e:
            return {
                'passed': False,
                'message': f"Integration validation error: {str(e)}",
                'exception': str(e)
            }

    def _generate_final_report(self) -> Dict[str, Any]:
        """Generate comprehensive final validation report"""
        total_tests = len(self.test_results)
        passed_tests = len([r for r in self.test_results.values() if r.get('passed', False)])
        failed_tests = total_tests - passed_tests

        overall_status = "PASS" if failed_tests == 0 else "FAIL"

        report = {
            'validation_timestamp': datetime.now().isoformat(),
            'overall_status': overall_status,
            'summary': {
                'total_tests': total_tests,
                'passed_tests': passed_tests,
                'failed_tests': failed_tests,
                'success_rate': (passed_tests / total_tests * 100) if total_tests > 0 else 0
            },
            'test_results': self.test_results,
            'validation_errors': self.validation_errors,
            'recommendations': self._generate_recommendations()
        }

        return report

    def _generate_recommendations(self) -> List[str]:
        """Generate recommendations based on validation results"""
        recommendations = []

        if not self.test_results.get('Configuration Validation', {}).get('passed', False):
            recommendations.append("Review and fix configuration parameters in .env file")

        if not self.test_results.get('MCP Bridge Enhancements', {}).get('passed', False):
            recommendations.append("Verify MCP bridge enhancements are properly implemented")

        if not self.test_results.get('Conversation Processing Fixes', {}).get('passed', False):
            recommendations.append("Check conversation processing function implementations")

        if not self.test_results.get('Session Management', {}).get('passed', False):
            recommendations.append("Review session management and cleanup logic")

        if not self.test_results.get('Error Handling', {}).get('passed', False):
            recommendations.append("Enhance error handling patterns in core modules")

        if not self.test_results.get('Integration Test', {}).get('passed', False):
            recommendations.append("Fix integration issues between components")

        if len(self.validation_errors) > 0:
            recommendations.append("Address all validation errors before deploying to production")

        if not recommendations:
            recommendations.append("All validations passed - fixes are ready for testing")

        return recommendations

async def main():
    """Main validation entry point"""
    print("🧪 Gemini 2.5 Tool Calling Fixes - Comprehensive Validation")
    print("=" * 60)

    validator = GeminiFixes Validator()

    try:
        final_report = await validator.run_all_validations()

        # Print summary
        print("\n📊 VALIDATION SUMMARY")
        print("-" * 30)
        print(f"Overall Status: {final_report['overall_status']}")
        print(f"Tests Passed: {final_report['summary']['passed_tests']}/{final_report['summary']['total_tests']}")
        print(f"Success Rate: {final_report['summary']['success_rate']:.1f}%")

        # Print recommendations
        if final_report['recommendations']:
            print("\n💡 RECOMMENDATIONS")
            print("-" * 20)
            for i, rec in enumerate(final_report['recommendations'], 1):
                print(f"{i}. {rec}")

        # Print detailed results if there are failures
        if final_report['validation_errors']:
            print("\n❌ VALIDATION ERRORS")
            print("-" * 20)
            for error in final_report['validation_errors']:
                print(f"• {error}")

        print(f"\n📅 Validation completed at: {final_report['validation_timestamp']}")

        # Return appropriate exit code
        return 0 if final_report['overall_status'] == 'PASS' else 1

    except Exception as e:
        print(f"\n💥 VALIDATION FRAMEWORK ERROR: {str(e)}")
        print(traceback.format_exc())
        return 2

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
